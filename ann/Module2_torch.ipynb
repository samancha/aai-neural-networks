{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nMpG2wtYW-cx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg_ssmpwXBHh",
        "outputId": "39fb0e60-3fbc-4780-cbda-215d0fa02956"
      },
      "outputs": [],
      "source": [
        "# Load the Boston Housing dataset\n",
        "data = load_boston()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZkZ1dybHXE2t"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yrswWzsVXNlc"
      },
      "outputs": [],
      "source": [
        "# Convert the numpy arrays to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uo-3PsqbXQ6j"
      },
      "outputs": [],
      "source": [
        "# Define the neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(13, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-LK9eqAIYx2x"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the neural network\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5a3nMHE1Y0Y8"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c69Y6ri9Y2nO",
        "outputId": "aa19cd51-8f27-4b10-e65e-02b4e8b12c60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/5000], Loss: 988.1300\n",
            "Epoch [20/5000], Loss: 962.9041\n",
            "Epoch [30/5000], Loss: 867.9314\n",
            "Epoch [40/5000], Loss: 786.0460\n",
            "Epoch [50/5000], Loss: 666.9530\n",
            "Epoch [60/5000], Loss: 625.4909\n",
            "Epoch [70/5000], Loss: 597.2298\n",
            "Epoch [80/5000], Loss: 533.6621\n",
            "Epoch [90/5000], Loss: 490.8483\n",
            "Epoch [100/5000], Loss: 478.7123\n",
            "Epoch [110/5000], Loss: 428.7823\n",
            "Epoch [120/5000], Loss: 473.5466\n",
            "Epoch [130/5000], Loss: 474.4965\n",
            "Epoch [140/5000], Loss: 391.2888\n",
            "Epoch [150/5000], Loss: 481.9718\n",
            "Epoch [160/5000], Loss: 404.1506\n",
            "Epoch [170/5000], Loss: 371.3592\n",
            "Epoch [180/5000], Loss: 345.9001\n",
            "Epoch [190/5000], Loss: 357.0353\n",
            "Epoch [200/5000], Loss: 332.9696\n",
            "Epoch [210/5000], Loss: 328.8810\n",
            "Epoch [220/5000], Loss: 338.6534\n",
            "Epoch [230/5000], Loss: 307.2007\n",
            "Epoch [240/5000], Loss: 371.2970\n",
            "Epoch [250/5000], Loss: 342.5326\n",
            "Epoch [260/5000], Loss: 324.1780\n",
            "Epoch [270/5000], Loss: 312.7422\n",
            "Epoch [280/5000], Loss: 333.8797\n",
            "Epoch [290/5000], Loss: 270.2287\n",
            "Epoch [300/5000], Loss: 278.4871\n",
            "Epoch [310/5000], Loss: 317.3236\n",
            "Epoch [320/5000], Loss: 296.9447\n",
            "Epoch [330/5000], Loss: 274.6066\n",
            "Epoch [340/5000], Loss: 335.9073\n",
            "Epoch [350/5000], Loss: 342.2591\n",
            "Epoch [360/5000], Loss: 321.9938\n",
            "Epoch [370/5000], Loss: 312.8758\n",
            "Epoch [380/5000], Loss: 284.4379\n",
            "Epoch [390/5000], Loss: 312.9386\n",
            "Epoch [400/5000], Loss: 332.9478\n",
            "Epoch [410/5000], Loss: 285.6133\n",
            "Epoch [420/5000], Loss: 286.3119\n",
            "Epoch [430/5000], Loss: 322.9449\n",
            "Epoch [440/5000], Loss: 251.5990\n",
            "Epoch [450/5000], Loss: 257.4597\n",
            "Epoch [460/5000], Loss: 281.1473\n",
            "Epoch [470/5000], Loss: 283.6463\n",
            "Epoch [480/5000], Loss: 305.5734\n",
            "Epoch [490/5000], Loss: 284.9252\n",
            "Epoch [500/5000], Loss: 263.2647\n",
            "Epoch [510/5000], Loss: 284.7172\n",
            "Epoch [520/5000], Loss: 255.5384\n",
            "Epoch [530/5000], Loss: 312.3718\n",
            "Epoch [540/5000], Loss: 222.1647\n",
            "Epoch [550/5000], Loss: 260.4907\n",
            "Epoch [560/5000], Loss: 260.5841\n",
            "Epoch [570/5000], Loss: 259.1436\n",
            "Epoch [580/5000], Loss: 261.5210\n",
            "Epoch [590/5000], Loss: 256.6121\n",
            "Epoch [600/5000], Loss: 245.0143\n",
            "Epoch [610/5000], Loss: 238.0378\n",
            "Epoch [620/5000], Loss: 243.9776\n",
            "Epoch [630/5000], Loss: 254.3583\n",
            "Epoch [640/5000], Loss: 230.4836\n",
            "Epoch [650/5000], Loss: 263.3106\n",
            "Epoch [660/5000], Loss: 215.5875\n",
            "Epoch [670/5000], Loss: 233.1218\n",
            "Epoch [680/5000], Loss: 248.5435\n",
            "Epoch [690/5000], Loss: 227.3062\n",
            "Epoch [700/5000], Loss: 222.4043\n",
            "Epoch [710/5000], Loss: 243.0650\n",
            "Epoch [720/5000], Loss: 225.1331\n",
            "Epoch [730/5000], Loss: 218.5616\n",
            "Epoch [740/5000], Loss: 228.6107\n",
            "Epoch [750/5000], Loss: 214.2544\n",
            "Epoch [760/5000], Loss: 235.1272\n",
            "Epoch [770/5000], Loss: 211.5033\n",
            "Epoch [780/5000], Loss: 212.6065\n",
            "Epoch [790/5000], Loss: 209.9191\n",
            "Epoch [800/5000], Loss: 204.7309\n",
            "Epoch [810/5000], Loss: 227.7907\n",
            "Epoch [820/5000], Loss: 208.9704\n",
            "Epoch [830/5000], Loss: 215.0976\n",
            "Epoch [840/5000], Loss: 222.6809\n",
            "Epoch [850/5000], Loss: 220.5816\n",
            "Epoch [860/5000], Loss: 226.4542\n",
            "Epoch [870/5000], Loss: 187.3394\n",
            "Epoch [880/5000], Loss: 202.2537\n",
            "Epoch [890/5000], Loss: 212.5876\n",
            "Epoch [900/5000], Loss: 242.3641\n",
            "Epoch [910/5000], Loss: 225.6144\n",
            "Epoch [920/5000], Loss: 216.5562\n",
            "Epoch [930/5000], Loss: 185.8608\n",
            "Epoch [940/5000], Loss: 221.1834\n",
            "Epoch [950/5000], Loss: 214.8602\n",
            "Epoch [960/5000], Loss: 194.3874\n",
            "Epoch [970/5000], Loss: 206.5399\n",
            "Epoch [980/5000], Loss: 214.0526\n",
            "Epoch [990/5000], Loss: 190.4301\n",
            "Epoch [1000/5000], Loss: 243.2888\n",
            "Epoch [1010/5000], Loss: 211.0367\n",
            "Epoch [1020/5000], Loss: 189.5223\n",
            "Epoch [1030/5000], Loss: 184.1041\n",
            "Epoch [1040/5000], Loss: 200.1300\n",
            "Epoch [1050/5000], Loss: 215.3810\n",
            "Epoch [1060/5000], Loss: 200.5858\n",
            "Epoch [1070/5000], Loss: 219.2799\n",
            "Epoch [1080/5000], Loss: 190.0340\n",
            "Epoch [1090/5000], Loss: 186.1798\n",
            "Epoch [1100/5000], Loss: 190.0588\n",
            "Epoch [1110/5000], Loss: 193.5198\n",
            "Epoch [1120/5000], Loss: 202.2933\n",
            "Epoch [1130/5000], Loss: 183.3954\n",
            "Epoch [1140/5000], Loss: 203.3894\n",
            "Epoch [1150/5000], Loss: 225.5226\n",
            "Epoch [1160/5000], Loss: 224.2812\n",
            "Epoch [1170/5000], Loss: 202.5041\n",
            "Epoch [1180/5000], Loss: 181.0307\n",
            "Epoch [1190/5000], Loss: 191.8307\n",
            "Epoch [1200/5000], Loss: 214.4835\n",
            "Epoch [1210/5000], Loss: 204.6978\n",
            "Epoch [1220/5000], Loss: 167.7916\n",
            "Epoch [1230/5000], Loss: 196.9097\n",
            "Epoch [1240/5000], Loss: 176.0615\n",
            "Epoch [1250/5000], Loss: 187.2298\n",
            "Epoch [1260/5000], Loss: 181.4549\n",
            "Epoch [1270/5000], Loss: 154.2212\n",
            "Epoch [1280/5000], Loss: 175.1914\n",
            "Epoch [1290/5000], Loss: 165.4327\n",
            "Epoch [1300/5000], Loss: 172.3526\n",
            "Epoch [1310/5000], Loss: 157.1032\n",
            "Epoch [1320/5000], Loss: 173.4835\n",
            "Epoch [1330/5000], Loss: 176.6885\n",
            "Epoch [1340/5000], Loss: 140.9000\n",
            "Epoch [1350/5000], Loss: 153.5398\n",
            "Epoch [1360/5000], Loss: 174.4917\n",
            "Epoch [1370/5000], Loss: 140.0744\n",
            "Epoch [1380/5000], Loss: 158.2665\n",
            "Epoch [1390/5000], Loss: 166.1929\n",
            "Epoch [1400/5000], Loss: 164.1097\n",
            "Epoch [1410/5000], Loss: 196.8048\n",
            "Epoch [1420/5000], Loss: 185.3918\n",
            "Epoch [1430/5000], Loss: 149.0894\n",
            "Epoch [1440/5000], Loss: 163.8649\n",
            "Epoch [1450/5000], Loss: 178.5217\n",
            "Epoch [1460/5000], Loss: 154.5176\n",
            "Epoch [1470/5000], Loss: 167.1359\n",
            "Epoch [1480/5000], Loss: 160.3937\n",
            "Epoch [1490/5000], Loss: 158.4449\n",
            "Epoch [1500/5000], Loss: 157.9670\n",
            "Epoch [1510/5000], Loss: 166.9129\n",
            "Epoch [1520/5000], Loss: 154.3864\n",
            "Epoch [1530/5000], Loss: 145.7776\n",
            "Epoch [1540/5000], Loss: 173.9048\n",
            "Epoch [1550/5000], Loss: 142.9897\n",
            "Epoch [1560/5000], Loss: 137.6880\n",
            "Epoch [1570/5000], Loss: 137.5167\n",
            "Epoch [1580/5000], Loss: 142.3226\n",
            "Epoch [1590/5000], Loss: 178.5724\n",
            "Epoch [1600/5000], Loss: 148.5858\n",
            "Epoch [1610/5000], Loss: 169.1699\n",
            "Epoch [1620/5000], Loss: 166.5772\n",
            "Epoch [1630/5000], Loss: 151.2979\n",
            "Epoch [1640/5000], Loss: 168.6326\n",
            "Epoch [1650/5000], Loss: 160.3527\n",
            "Epoch [1660/5000], Loss: 156.9938\n",
            "Epoch [1670/5000], Loss: 144.1256\n",
            "Epoch [1680/5000], Loss: 160.8595\n",
            "Epoch [1690/5000], Loss: 155.5604\n",
            "Epoch [1700/5000], Loss: 154.1662\n",
            "Epoch [1710/5000], Loss: 152.5620\n",
            "Epoch [1720/5000], Loss: 144.8591\n",
            "Epoch [1730/5000], Loss: 150.6300\n",
            "Epoch [1740/5000], Loss: 180.2027\n",
            "Epoch [1750/5000], Loss: 161.5684\n",
            "Epoch [1760/5000], Loss: 143.1772\n",
            "Epoch [1770/5000], Loss: 142.3123\n",
            "Epoch [1780/5000], Loss: 155.4514\n",
            "Epoch [1790/5000], Loss: 144.5677\n",
            "Epoch [1800/5000], Loss: 138.6661\n",
            "Epoch [1810/5000], Loss: 148.0795\n",
            "Epoch [1820/5000], Loss: 188.9970\n",
            "Epoch [1830/5000], Loss: 125.0269\n",
            "Epoch [1840/5000], Loss: 132.4577\n",
            "Epoch [1850/5000], Loss: 138.6416\n",
            "Epoch [1860/5000], Loss: 154.0135\n",
            "Epoch [1870/5000], Loss: 155.1430\n",
            "Epoch [1880/5000], Loss: 131.6546\n",
            "Epoch [1890/5000], Loss: 137.6176\n",
            "Epoch [1900/5000], Loss: 144.5707\n",
            "Epoch [1910/5000], Loss: 149.2703\n",
            "Epoch [1920/5000], Loss: 170.3126\n",
            "Epoch [1930/5000], Loss: 155.6345\n",
            "Epoch [1940/5000], Loss: 165.4335\n",
            "Epoch [1950/5000], Loss: 151.3415\n",
            "Epoch [1960/5000], Loss: 145.9040\n",
            "Epoch [1970/5000], Loss: 124.1534\n",
            "Epoch [1980/5000], Loss: 147.2391\n",
            "Epoch [1990/5000], Loss: 148.9534\n",
            "Epoch [2000/5000], Loss: 152.7433\n",
            "Epoch [2010/5000], Loss: 135.2141\n",
            "Epoch [2020/5000], Loss: 138.0336\n",
            "Epoch [2030/5000], Loss: 112.7254\n",
            "Epoch [2040/5000], Loss: 153.2129\n",
            "Epoch [2050/5000], Loss: 152.1283\n",
            "Epoch [2060/5000], Loss: 153.3978\n",
            "Epoch [2070/5000], Loss: 131.3237\n",
            "Epoch [2080/5000], Loss: 126.2559\n",
            "Epoch [2090/5000], Loss: 131.4536\n",
            "Epoch [2100/5000], Loss: 131.4322\n",
            "Epoch [2110/5000], Loss: 129.3147\n",
            "Epoch [2120/5000], Loss: 147.2697\n",
            "Epoch [2130/5000], Loss: 129.2288\n",
            "Epoch [2140/5000], Loss: 134.6344\n",
            "Epoch [2150/5000], Loss: 125.5546\n",
            "Epoch [2160/5000], Loss: 154.3007\n",
            "Epoch [2170/5000], Loss: 145.1678\n",
            "Epoch [2180/5000], Loss: 139.6069\n",
            "Epoch [2190/5000], Loss: 133.7874\n",
            "Epoch [2200/5000], Loss: 168.3229\n",
            "Epoch [2210/5000], Loss: 142.1013\n",
            "Epoch [2220/5000], Loss: 123.5764\n",
            "Epoch [2230/5000], Loss: 127.6512\n",
            "Epoch [2240/5000], Loss: 132.1785\n",
            "Epoch [2250/5000], Loss: 133.0373\n",
            "Epoch [2260/5000], Loss: 123.9721\n",
            "Epoch [2270/5000], Loss: 120.7442\n",
            "Epoch [2280/5000], Loss: 122.3593\n",
            "Epoch [2290/5000], Loss: 128.1493\n",
            "Epoch [2300/5000], Loss: 138.5894\n",
            "Epoch [2310/5000], Loss: 134.8008\n",
            "Epoch [2320/5000], Loss: 129.5640\n",
            "Epoch [2330/5000], Loss: 112.8826\n",
            "Epoch [2340/5000], Loss: 125.3775\n",
            "Epoch [2350/5000], Loss: 138.4411\n",
            "Epoch [2360/5000], Loss: 109.7513\n",
            "Epoch [2370/5000], Loss: 114.5556\n",
            "Epoch [2380/5000], Loss: 129.9094\n",
            "Epoch [2390/5000], Loss: 116.9111\n",
            "Epoch [2400/5000], Loss: 136.3962\n",
            "Epoch [2410/5000], Loss: 116.8010\n",
            "Epoch [2420/5000], Loss: 161.9595\n",
            "Epoch [2430/5000], Loss: 125.1048\n",
            "Epoch [2440/5000], Loss: 100.4882\n",
            "Epoch [2450/5000], Loss: 128.1294\n",
            "Epoch [2460/5000], Loss: 130.3916\n",
            "Epoch [2470/5000], Loss: 122.6536\n",
            "Epoch [2480/5000], Loss: 126.7770\n",
            "Epoch [2490/5000], Loss: 129.3743\n",
            "Epoch [2500/5000], Loss: 136.8658\n",
            "Epoch [2510/5000], Loss: 156.8321\n",
            "Epoch [2520/5000], Loss: 115.6008\n",
            "Epoch [2530/5000], Loss: 127.4439\n",
            "Epoch [2540/5000], Loss: 142.1782\n",
            "Epoch [2550/5000], Loss: 120.9611\n",
            "Epoch [2560/5000], Loss: 122.7355\n",
            "Epoch [2570/5000], Loss: 101.4649\n",
            "Epoch [2580/5000], Loss: 128.6378\n",
            "Epoch [2590/5000], Loss: 149.5159\n",
            "Epoch [2600/5000], Loss: 113.8056\n",
            "Epoch [2610/5000], Loss: 118.8043\n",
            "Epoch [2620/5000], Loss: 121.0535\n",
            "Epoch [2630/5000], Loss: 119.1170\n",
            "Epoch [2640/5000], Loss: 116.5189\n",
            "Epoch [2650/5000], Loss: 116.3148\n",
            "Epoch [2660/5000], Loss: 121.3251\n",
            "Epoch [2670/5000], Loss: 105.6396\n",
            "Epoch [2680/5000], Loss: 111.1010\n",
            "Epoch [2690/5000], Loss: 127.7859\n",
            "Epoch [2700/5000], Loss: 108.9346\n",
            "Epoch [2710/5000], Loss: 126.3767\n",
            "Epoch [2720/5000], Loss: 105.6429\n",
            "Epoch [2730/5000], Loss: 122.5860\n",
            "Epoch [2740/5000], Loss: 110.9668\n",
            "Epoch [2750/5000], Loss: 103.4159\n",
            "Epoch [2760/5000], Loss: 132.9293\n",
            "Epoch [2770/5000], Loss: 106.5667\n",
            "Epoch [2780/5000], Loss: 102.9498\n",
            "Epoch [2790/5000], Loss: 101.8189\n",
            "Epoch [2800/5000], Loss: 118.0944\n",
            "Epoch [2810/5000], Loss: 120.9075\n",
            "Epoch [2820/5000], Loss: 114.2297\n",
            "Epoch [2830/5000], Loss: 135.4798\n",
            "Epoch [2840/5000], Loss: 122.5574\n",
            "Epoch [2850/5000], Loss: 125.5290\n",
            "Epoch [2860/5000], Loss: 110.2225\n",
            "Epoch [2870/5000], Loss: 113.6373\n",
            "Epoch [2880/5000], Loss: 138.6838\n",
            "Epoch [2890/5000], Loss: 120.1775\n",
            "Epoch [2900/5000], Loss: 120.1749\n",
            "Epoch [2910/5000], Loss: 101.2198\n",
            "Epoch [2920/5000], Loss: 117.1874\n",
            "Epoch [2930/5000], Loss: 127.1435\n",
            "Epoch [2940/5000], Loss: 115.7493\n",
            "Epoch [2950/5000], Loss: 106.7740\n",
            "Epoch [2960/5000], Loss: 107.9410\n",
            "Epoch [2970/5000], Loss: 119.5989\n",
            "Epoch [2980/5000], Loss: 105.7296\n",
            "Epoch [2990/5000], Loss: 116.8949\n",
            "Epoch [3000/5000], Loss: 101.7769\n",
            "Epoch [3010/5000], Loss: 118.7717\n",
            "Epoch [3020/5000], Loss: 118.9000\n",
            "Epoch [3030/5000], Loss: 98.7208\n",
            "Epoch [3040/5000], Loss: 134.0082\n",
            "Epoch [3050/5000], Loss: 127.6854\n",
            "Epoch [3060/5000], Loss: 114.8417\n",
            "Epoch [3070/5000], Loss: 122.9626\n",
            "Epoch [3080/5000], Loss: 166.7295\n",
            "Epoch [3090/5000], Loss: 101.1863\n",
            "Epoch [3100/5000], Loss: 147.4311\n",
            "Epoch [3110/5000], Loss: 92.5246\n",
            "Epoch [3120/5000], Loss: 107.1372\n",
            "Epoch [3130/5000], Loss: 109.9082\n",
            "Epoch [3140/5000], Loss: 104.9919\n",
            "Epoch [3150/5000], Loss: 103.5612\n",
            "Epoch [3160/5000], Loss: 108.4190\n",
            "Epoch [3170/5000], Loss: 101.9021\n",
            "Epoch [3180/5000], Loss: 108.7893\n",
            "Epoch [3190/5000], Loss: 114.2027\n",
            "Epoch [3200/5000], Loss: 107.1704\n",
            "Epoch [3210/5000], Loss: 104.0307\n",
            "Epoch [3220/5000], Loss: 116.2215\n",
            "Epoch [3230/5000], Loss: 106.4561\n",
            "Epoch [3240/5000], Loss: 105.7961\n",
            "Epoch [3250/5000], Loss: 94.5765\n",
            "Epoch [3260/5000], Loss: 94.4003\n",
            "Epoch [3270/5000], Loss: 102.6183\n",
            "Epoch [3280/5000], Loss: 167.0065\n",
            "Epoch [3290/5000], Loss: 110.2387\n",
            "Epoch [3300/5000], Loss: 128.8037\n",
            "Epoch [3310/5000], Loss: 105.7952\n",
            "Epoch [3320/5000], Loss: 101.9409\n",
            "Epoch [3330/5000], Loss: 122.6966\n",
            "Epoch [3340/5000], Loss: 117.1645\n",
            "Epoch [3350/5000], Loss: 103.7370\n",
            "Epoch [3360/5000], Loss: 107.6204\n",
            "Epoch [3370/5000], Loss: 110.9741\n",
            "Epoch [3380/5000], Loss: 100.4732\n",
            "Epoch [3390/5000], Loss: 106.8723\n",
            "Epoch [3400/5000], Loss: 108.0021\n",
            "Epoch [3410/5000], Loss: 92.2584\n",
            "Epoch [3420/5000], Loss: 111.8499\n",
            "Epoch [3430/5000], Loss: 105.2196\n",
            "Epoch [3440/5000], Loss: 112.0244\n",
            "Epoch [3450/5000], Loss: 124.0272\n",
            "Epoch [3460/5000], Loss: 105.3821\n",
            "Epoch [3470/5000], Loss: 115.4540\n",
            "Epoch [3480/5000], Loss: 98.0451\n",
            "Epoch [3490/5000], Loss: 118.9798\n",
            "Epoch [3500/5000], Loss: 100.8466\n",
            "Epoch [3510/5000], Loss: 96.9725\n",
            "Epoch [3520/5000], Loss: 103.1049\n",
            "Epoch [3530/5000], Loss: 110.1931\n",
            "Epoch [3540/5000], Loss: 124.3361\n",
            "Epoch [3550/5000], Loss: 102.7379\n",
            "Epoch [3560/5000], Loss: 109.9826\n",
            "Epoch [3570/5000], Loss: 136.7805\n",
            "Epoch [3580/5000], Loss: 110.3037\n",
            "Epoch [3590/5000], Loss: 122.5568\n",
            "Epoch [3600/5000], Loss: 94.5406\n",
            "Epoch [3610/5000], Loss: 95.1448\n",
            "Epoch [3620/5000], Loss: 110.8308\n",
            "Epoch [3630/5000], Loss: 98.6826\n",
            "Epoch [3640/5000], Loss: 97.3342\n",
            "Epoch [3650/5000], Loss: 93.1629\n",
            "Epoch [3660/5000], Loss: 111.2816\n",
            "Epoch [3670/5000], Loss: 108.4028\n",
            "Epoch [3680/5000], Loss: 113.3864\n",
            "Epoch [3690/5000], Loss: 114.1855\n",
            "Epoch [3700/5000], Loss: 142.7351\n",
            "Epoch [3710/5000], Loss: 120.8293\n",
            "Epoch [3720/5000], Loss: 95.4261\n",
            "Epoch [3730/5000], Loss: 106.6390\n",
            "Epoch [3740/5000], Loss: 106.8168\n",
            "Epoch [3750/5000], Loss: 115.2133\n",
            "Epoch [3760/5000], Loss: 118.8650\n",
            "Epoch [3770/5000], Loss: 118.4302\n",
            "Epoch [3780/5000], Loss: 111.2119\n",
            "Epoch [3790/5000], Loss: 101.2946\n",
            "Epoch [3800/5000], Loss: 96.9349\n",
            "Epoch [3810/5000], Loss: 117.3017\n",
            "Epoch [3820/5000], Loss: 108.3997\n",
            "Epoch [3830/5000], Loss: 123.8498\n",
            "Epoch [3840/5000], Loss: 151.2224\n",
            "Epoch [3850/5000], Loss: 91.1680\n",
            "Epoch [3860/5000], Loss: 107.0917\n",
            "Epoch [3870/5000], Loss: 97.0693\n",
            "Epoch [3880/5000], Loss: 111.0147\n",
            "Epoch [3890/5000], Loss: 96.9972\n",
            "Epoch [3900/5000], Loss: 115.1957\n",
            "Epoch [3910/5000], Loss: 104.9203\n",
            "Epoch [3920/5000], Loss: 107.6033\n",
            "Epoch [3930/5000], Loss: 95.2168\n",
            "Epoch [3940/5000], Loss: 104.1715\n",
            "Epoch [3950/5000], Loss: 115.2667\n",
            "Epoch [3960/5000], Loss: 96.2487\n",
            "Epoch [3970/5000], Loss: 111.6117\n",
            "Epoch [3980/5000], Loss: 97.4199\n",
            "Epoch [3990/5000], Loss: 105.9052\n",
            "Epoch [4000/5000], Loss: 105.7360\n",
            "Epoch [4010/5000], Loss: 96.8514\n",
            "Epoch [4020/5000], Loss: 103.8863\n",
            "Epoch [4030/5000], Loss: 88.1818\n",
            "Epoch [4040/5000], Loss: 93.3984\n",
            "Epoch [4050/5000], Loss: 95.4896\n",
            "Epoch [4060/5000], Loss: 95.4255\n",
            "Epoch [4070/5000], Loss: 120.1958\n",
            "Epoch [4080/5000], Loss: 100.3277\n",
            "Epoch [4090/5000], Loss: 120.4723\n",
            "Epoch [4100/5000], Loss: 88.0381\n",
            "Epoch [4110/5000], Loss: 92.3522\n",
            "Epoch [4120/5000], Loss: 104.7459\n",
            "Epoch [4130/5000], Loss: 85.9933\n",
            "Epoch [4140/5000], Loss: 87.8913\n",
            "Epoch [4150/5000], Loss: 132.6951\n",
            "Epoch [4160/5000], Loss: 91.6532\n",
            "Epoch [4170/5000], Loss: 103.3934\n",
            "Epoch [4180/5000], Loss: 97.7523\n",
            "Epoch [4190/5000], Loss: 103.2906\n",
            "Epoch [4200/5000], Loss: 102.7211\n",
            "Epoch [4210/5000], Loss: 104.5209\n",
            "Epoch [4220/5000], Loss: 114.9444\n",
            "Epoch [4230/5000], Loss: 98.2558\n",
            "Epoch [4240/5000], Loss: 82.4559\n",
            "Epoch [4250/5000], Loss: 98.3875\n",
            "Epoch [4260/5000], Loss: 107.8912\n",
            "Epoch [4270/5000], Loss: 102.8367\n",
            "Epoch [4280/5000], Loss: 102.6748\n",
            "Epoch [4290/5000], Loss: 102.9681\n",
            "Epoch [4300/5000], Loss: 114.9846\n",
            "Epoch [4310/5000], Loss: 93.0016\n",
            "Epoch [4320/5000], Loss: 95.3139\n",
            "Epoch [4330/5000], Loss: 101.2310\n",
            "Epoch [4340/5000], Loss: 96.1617\n",
            "Epoch [4350/5000], Loss: 113.7560\n",
            "Epoch [4360/5000], Loss: 103.6095\n",
            "Epoch [4370/5000], Loss: 95.5050\n",
            "Epoch [4380/5000], Loss: 98.6761\n",
            "Epoch [4390/5000], Loss: 108.8581\n",
            "Epoch [4400/5000], Loss: 108.5936\n",
            "Epoch [4410/5000], Loss: 95.1508\n",
            "Epoch [4420/5000], Loss: 93.2214\n",
            "Epoch [4430/5000], Loss: 101.7833\n",
            "Epoch [4440/5000], Loss: 88.8579\n",
            "Epoch [4450/5000], Loss: 110.3963\n",
            "Epoch [4460/5000], Loss: 108.7934\n",
            "Epoch [4470/5000], Loss: 88.4546\n",
            "Epoch [4480/5000], Loss: 105.1619\n",
            "Epoch [4490/5000], Loss: 105.5546\n",
            "Epoch [4500/5000], Loss: 111.5792\n",
            "Epoch [4510/5000], Loss: 97.5971\n",
            "Epoch [4520/5000], Loss: 81.4448\n",
            "Epoch [4530/5000], Loss: 82.9272\n",
            "Epoch [4540/5000], Loss: 121.3571\n",
            "Epoch [4550/5000], Loss: 101.8202\n",
            "Epoch [4560/5000], Loss: 96.2678\n",
            "Epoch [4570/5000], Loss: 97.6508\n",
            "Epoch [4580/5000], Loss: 101.4230\n",
            "Epoch [4590/5000], Loss: 99.0902\n",
            "Epoch [4600/5000], Loss: 98.9686\n",
            "Epoch [4610/5000], Loss: 98.5634\n",
            "Epoch [4620/5000], Loss: 106.8618\n",
            "Epoch [4630/5000], Loss: 108.9981\n",
            "Epoch [4640/5000], Loss: 86.0754\n",
            "Epoch [4650/5000], Loss: 103.8038\n",
            "Epoch [4660/5000], Loss: 95.6184\n",
            "Epoch [4670/5000], Loss: 92.4551\n",
            "Epoch [4680/5000], Loss: 95.0022\n",
            "Epoch [4690/5000], Loss: 93.2840\n",
            "Epoch [4700/5000], Loss: 87.8984\n",
            "Epoch [4710/5000], Loss: 110.3124\n",
            "Epoch [4720/5000], Loss: 91.0658\n",
            "Epoch [4730/5000], Loss: 105.9807\n",
            "Epoch [4740/5000], Loss: 86.3094\n",
            "Epoch [4750/5000], Loss: 86.4180\n",
            "Epoch [4760/5000], Loss: 84.6246\n",
            "Epoch [4770/5000], Loss: 91.8832\n",
            "Epoch [4780/5000], Loss: 83.1940\n",
            "Epoch [4790/5000], Loss: 101.4829\n",
            "Epoch [4800/5000], Loss: 71.7195\n",
            "Epoch [4810/5000], Loss: 83.4308\n",
            "Epoch [4820/5000], Loss: 113.4571\n",
            "Epoch [4830/5000], Loss: 95.3043\n",
            "Epoch [4840/5000], Loss: 100.3269\n",
            "Epoch [4850/5000], Loss: 106.2627\n",
            "Epoch [4860/5000], Loss: 92.5455\n",
            "Epoch [4870/5000], Loss: 86.6774\n",
            "Epoch [4880/5000], Loss: 101.7420\n",
            "Epoch [4890/5000], Loss: 87.6043\n",
            "Epoch [4900/5000], Loss: 84.9462\n",
            "Epoch [4910/5000], Loss: 91.6886\n",
            "Epoch [4920/5000], Loss: 95.4372\n",
            "Epoch [4930/5000], Loss: 95.9485\n",
            "Epoch [4940/5000], Loss: 91.0725\n",
            "Epoch [4950/5000], Loss: 92.6484\n",
            "Epoch [4960/5000], Loss: 94.2358\n",
            "Epoch [4970/5000], Loss: 93.1234\n",
            "Epoch [4980/5000], Loss: 101.6769\n",
            "Epoch [4990/5000], Loss: 108.1260\n",
            "Epoch [5000/5000], Loss: 89.4191\n"
          ]
        }
      ],
      "source": [
        "# Train the neural network\n",
        "num_epochs = 5000\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(X_train[i:i+batch_size])\n",
        "        loss = criterion(outputs, y_train[i:i+batch_size].unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, running_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwXC2MB2Y6Eb",
        "outputId": "921a5036-70c6-473f-82db-2a6ab36cb58c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 21.5320\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the testing set\n",
        "with torch.no_grad():\n",
        "    y_pred = net(X_test)\n",
        "    test_loss = criterion(y_pred, y_test.unsqueeze(1))\n",
        "    print('Test Loss: {:.4f}'.format(test_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AMEkBCUtY9QU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
